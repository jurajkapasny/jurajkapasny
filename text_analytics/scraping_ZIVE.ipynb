{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "data_path = \"/Users/jurajkapasny/Data/sk_text_for_api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ScrapeZIVE:\n",
    "    def __init__(self):\n",
    "        # Base soup object\n",
    "        self.page = requests.get(\"http://www.zive.sk\")\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        \n",
    "        # I will be adding links here constantly so needs to be class variable\n",
    "        # loading old links from disk\n",
    "        try:\n",
    "            self.scraped_links = list(pd.read_csv(data_path + \"zive_links.csv\", sep = \"|\").links.values)\n",
    "            print len(self.scraped_links), \"links loaded from disk\"\n",
    "        except IOError:    \n",
    "            # Empty link list if no data are on the disk\n",
    "            self.scraped_links = []\n",
    "    \n",
    "    def find_body(self, soup = None):\n",
    "        if soup is None:\n",
    "            soup = self.soup\n",
    "        main_content = list(soup.children)\n",
    "        for i in main_content:\n",
    "            string = i.encode('utf-8')\n",
    "            #ked zacina cast <html tak to je main part\n",
    "            if string.find(\"<html\") == 0:\n",
    "                inner_content = i\n",
    "                for j in inner_content:\n",
    "                    inner_string = j.encode('utf-8')\n",
    "                    if inner_string.find(\"<body\") == 0:\n",
    "                        body = j\n",
    "                        print \"Find HTML <body>: Successfull!!\"\n",
    "        return body\n",
    "    \n",
    "    def find_initial_links(self):\n",
    "        article_links = []\n",
    "        summaries = []\n",
    "        body = self.find_body()\n",
    "        for i in body.select('ul.entry-content div.boxItemWrapper a.boxItemLink'):\n",
    "            link = str(i.get(\"href\"))\n",
    "            if link.find(\"?\") != -1:\n",
    "                link = link[:link.find(\"?\")]\n",
    "            summaries.append(i.select('span.articleTitle')[0].get_text())\n",
    "            article_links.append(link)\n",
    "        return article_links, summaries\n",
    "            \n",
    "    def scrape(self):\n",
    "        # initial links to scrape\n",
    "        article_links, summaries = self.find_initial_links()\n",
    "        self.sme_articles = {}\n",
    "        i = 0\n",
    "#         while i < len(article_links):\n",
    "        while i < 3:\n",
    "            print \"Number of articles for download:\",len(article_links)\n",
    "            print \"Scraping article number\", i+1\n",
    "            link = article_links[i]\n",
    "            summary = summaries[i]\n",
    "            # skip this interation if link is already parsed\n",
    "            if link in self.scraped_links:\n",
    "                print \"Link already scraped, skipping...!\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                continue\n",
    "            \n",
    "            # sometimes link doesn't work\n",
    "            try:\n",
    "                article = requests.get(link)\n",
    "            except:\n",
    "                print \"link was corrupted\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                # skips rest of the loop\n",
    "                continue\n",
    "            \n",
    "            local_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "            # find HTML <body> tag\n",
    "            body = self.find_body(soup = local_soup)\n",
    "            \n",
    "            # label is group of news from webpage\n",
    "#             label = re.search('//(.*).zive.sk', link).group(1)\n",
    "            \n",
    "            \n",
    "            text = \"\"\n",
    "            # identification of text of the article\n",
    "            for article_part in body.select('div.articleFulltext p'):\n",
    "                 text = text + \"\\n\" + article_part.get_text()\n",
    "#             self.sme_articles[i] = (label, text)\n",
    "            self.sme_articles[i] = (summary, text)\n",
    "            \n",
    "            #identification of next articles:\n",
    "#             for next_article in body.select('div.media-body h2.media-heading a'):\n",
    "#                 # getting new links from current article\n",
    "#                 new_link = str(next_article.get(\"href\"))\n",
    "#                 # sometimes, there is a source page after the \"?\" leading to duplicates in the link\n",
    "#                 if new_link.find(\"?\") != -1:\n",
    "#                     # removing part after the \"?\"\n",
    "#                     new_link = new_link[:new_link.find(\"?\")]\n",
    "#                 # if new_link is not in the list, then it will be added\n",
    "#                 if new_link not in article_links:\n",
    "#                     article_links.append(new_link)\n",
    "                    \n",
    "#             #identification of next articles:\n",
    "#             for next_article in body.select('div.cr-box div.cr-content h4 a'):\n",
    "#                 new_link = str(next_article.get(\"href\"))\n",
    "#                 if new_link.find(\"?\") != -1:\n",
    "#                     new_link = new_link[:new_link.find(\"?\")]\n",
    "#                 if new_link not in article_links:\n",
    "#                     article_links.append(new_link)\n",
    "            \n",
    "            i = i + 1\n",
    "            self.scraped_links.append(link)\n",
    "            print \"Done!!\"\n",
    "            print \"\"\n",
    "            \n",
    "#             # Saving to disk temp results\n",
    "#             if (i%100 == 0) & (len(self.sme_articles) > 1):\n",
    "#                 print \"Saving partial results to disk!\"\n",
    "#                 df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "#                 df_articles.columns = [\"category\",\"text\"]\n",
    "#                 df_articles.to_csv(data_path + \"temp_sme_articles.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "#                 df_links = pd.DataFrame(self.scraped_links)\n",
    "#                 df_links.columns = [\"links\"]\n",
    "#                 df_links.to_csv(data_path + \"temp_sme_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        \n",
    "#         # Creating final dataframe and saving to disk\n",
    "#         # TODO better savings\n",
    "#         if len(self.sme_articles) > 1:\n",
    "#             postfix = str(time.time())\n",
    "#             if postfix.find(\".\") != -1:\n",
    "#                 postfix = postfix[:postfix.find(\".\")]\n",
    "            \n",
    "#             print \"Saving final results to disk!\"\n",
    "#             df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "#             df_articles.columns = [\"category\",\"text\"]\n",
    "#             df_articles.to_csv(data_path + \"sme_articles\"+ postfix + \".csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "#             df_links = pd.DataFrame(self.scraped_links)\n",
    "#             df_links.columns = [\"links\"]\n",
    "#             df_links.to_csv(data_path + \"sme_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "#         else:\n",
    "#             print \"No new articles scraped\"\n",
    "        \n",
    "        return self.sme_articles, self.scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find HTML <body>: Successfull!!\n",
      "Number of articles for download: 95\n",
      "Scraping article number 1\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 95\n",
      "Scraping article number 2\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 95\n",
      "Scraping article number 3\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = ScrapeZIVE()\n",
    "articles, links = x.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ministerstvo spravodlivosti USA sa opäť púšťa do útoku na Microsoft. Po minuloročnom úspešnom odvolaní v prospech redmondskej spoločnosti, týkajúcej sa poskytnutia dát zo zahraničných serverov pripravuje žiadosť o preskúmanie rozhodnutia Najvyšším súdom.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tVíťazstvo pre Microsoft. Spoločnosť nemusí americkej vláde poskytnúť dáta z európskych serverov\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\tVláda si v rámci vyšetrovania drogového prípadu žiadala prístup k írskym serverom. Súd to však zamietol.\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tVláda si v rámci vyšetrovania drogového prípadu žiadala prístup k írskym serverom. Súd to však zamietol.\t\t\t\t\t\n",
      "Pripomeňme, že išlo o vyšetrovanie istého drogového dílera, ktorý komunikoval prostredníctvom Hotmailu a jeho schránka bola umiestnená na írskom serveri Microsoftu. Firma sa vtedy odmietla podvoliť, pretože podľa jej slov bola e-mailová schránka mimo právomoc amerického súdu. Povolenie k prístupu do schránky by museli vyšetrovateľom udeliť aj írske úrady.\n",
      "Hoci ministerstvo najskôr zvíťazilo, ďalší súd už dal za pravdu Microsoftu. Až do preniknutia plánov amerického ministerstva na verejnosť sa zdalo, že sa tým celá kauza uzavrie.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tMicrosoft nechce poskytovať dáta o používateľoch vláde. Na súde ho podporujú Apple aj Google\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\tZákon, ktorým sa riadi americké ministerstvo spravodlivosti, údajne porušuje štvrtý dodatok ústavy. Štát na konanie súdu nevidí dôvod.\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tZákon, ktorým sa riadi americké ministerstvo spravodlivosti, údajne porušuje štvrtý dodatok ústavy. Štát na konanie súdu nevidí dôvod.\t\t\t\t\t\n",
      "Microsoft sa proti nemu ostro postavil na svojom blogu. Firma by totiž opäť riskovala vyšetrovanie v Európskej únii. Na budúcu jar totiž na starom kontinente vstúpi do platnosti legislatíva GDPR (General Data Protection Regulation, vo voľnom preklade Všeobecná regulácia ochrany dát), v rámci ktorej bude pre spoločnosti ilegálne poskytovať dáta používateľov na základe jednostranného súdneho príkazu z cudzej krajiny.\n",
      "\n",
      "\n",
      "        googletag.cmd.push(function() { googletag.display('div-gpt-ad-zive-desktop-square-inline'); });\n",
      "    \n",
      "\n",
      "Microsoft v príspevku zmieňuje, že by sa tak mohol dostať do celkom chúlostivej situácie. Keď nevyhovie americkým súdom, stihne ho trest v USA. V opačnom prípade by ho zrejme neminula pokuta v EÚ.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tPísmeno J namiesto smajlíka v e-mailoch? Microsoft to konečne vyriešil\t\t\t\t\t\n",
      "\n",
      "\t\t\t\t\t\tSpoločnosti trvalo sedem rokov, než do svojho e-mailového klienta správne implementovala podporu emotikonov.\t\t\t\t\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tSpoločnosti trvalo sedem rokov, než do svojho e-mailového klienta správne implementovala podporu emotikonov.\t\t\t\t\t\n",
      "Firma preto po vzore Európy navrhuje novelu amerického práva. Tamojšie ministerstvo sa totiž snaží od Microsoftu získať dáta spomínaného kriminálnika na základe viac ako tridsať rokov starého zákona. V čase jeho zavedenie bola samozrejme iná doba a cezhraničný internet v takej miere ako dnes ešte neexistoval.\n"
     ]
    }
   ],
   "source": [
    "print articles[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/jurajkapasny/Data/sk_text_for_api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    scraper = ScrapeSME()\n",
    "    articles, links = scraper.scrape()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    minutes = int(elapsed / 60)\n",
    "    seconds = elapsed % 60\n",
    "    print \"Number of scraped articles:\", len(articles)\n",
    "    print \"Time Elapsed:\", minutes, \"minutes and\", seconds, \"seconds\"\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
