{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ScrapePravda:\n",
    "    def __init__(self):\n",
    "        # Base soup object\n",
    "        self.page = requests.get(\"http://www.pravda.sk\")\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        \n",
    "        # I will be adding links here constantly so needs to be class variable\n",
    "        # loading old links from disk\n",
    "        try:\n",
    "            self.scraped_links = list(pd.read_csv(data_path + \"pravda_links.csv\", sep = \"|\").links.values)\n",
    "            print len(self.scraped_links), \"links loaded from disk\"\n",
    "        except IOError:    \n",
    "            # Empty link list if no data are on the disk\n",
    "            self.scraped_links = []\n",
    "#         self.scraped_links = []\n",
    "    \n",
    "    def find_body(self, soup = None):\n",
    "        if soup is None:\n",
    "            soup = self.soup\n",
    "        main_content = list(soup.children)\n",
    "        for i in main_content:\n",
    "            #ked zacina cast <html tak to je main part\n",
    "            if str(i).find(\"<html\") == 0:\n",
    "                inner_content = i\n",
    "                for j in inner_content:\n",
    "                    if str(j).find(\"<body\") == 0:\n",
    "                        body = j\n",
    "                        print \"Find HTML <body>: Successfull!!\"\n",
    "        if 'body' not in locals():\n",
    "            print \"body wasn't find\"\n",
    "            body = None\n",
    "        return body\n",
    "    \n",
    "    def find_initial_links(self):\n",
    "        article_links = []\n",
    "        body = self.find_body()\n",
    "        for i in body.select('section.hp-box-spravy div.article-square div.no-padding-side'):\n",
    "            # link is always first elementh in the paragraph, sometimes is missings (taking from minuta po minute)\n",
    "            try:\n",
    "                link = str(i.select('h4 a')[0].get(\"href\"))\n",
    "            except IndexError:\n",
    "                continue\n",
    "            if link.find(\"?\") != -1:\n",
    "                link = link[:link.find(\"?\")]\n",
    "\n",
    "            article_links.append(link)\n",
    "    \n",
    "        return article_links\n",
    "            \n",
    "    def scrape(self):\n",
    "        # initial links to scrape\n",
    "        article_links = self.find_initial_links()\n",
    "        self.sme_articles = {}\n",
    "        i = 0\n",
    "        while i < len(article_links):\n",
    "        #while i < 4:\n",
    "            print \"Number of articles for download:\",len(article_links)\n",
    "            print \"Scraping article number\", i+1\n",
    "            link = article_links[i]\n",
    "            print link            \n",
    "            # skip this interation if link is already parsed\n",
    "            if link in self.scraped_links:\n",
    "                print \"Link already scraped, skipping...!\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                continue\n",
    "            \n",
    "            # sometimes link doesn't work\n",
    "            try:\n",
    "                article = requests.get(link)\n",
    "            except:\n",
    "                print \"link was corrupted\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                # skips rest of the loop\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            local_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "            # find HTML <body> tag\n",
    "            body = self.find_body(soup = local_soup)\n",
    "            if body == None:\n",
    "                print \"link was corrupted\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                # skips rest of the loop\n",
    "                continue\n",
    "            # label is group of news from webpage\n",
    "            # label is group of news from webpage\n",
    "            try:\n",
    "                label = re.search('.pravda.sk/(.*?)/', link).group(1)\n",
    "            except AttributeError:\n",
    "                label = \"\"\n",
    "\n",
    "            # summary is in the first paragraph\n",
    "            for desc in body.select('article.article-detail div.article-detail-perex p'):\n",
    "                 summary = desc.get_text()\n",
    "            # identification of text of the article\n",
    "            text = \"\"\n",
    "            for article_part in body.select('article.article-detail div.article-detail-body p'):\n",
    "                 text = text + \"\\n\" + article_part.get_text()\n",
    "            \n",
    "            \n",
    "            self.sme_articles[i] = (label,summary, text)\n",
    "            \n",
    "            #identification of next articles:\n",
    "            for next_article in body.select('aside.article-detail-body-odporucame div.article-head h3 a'):\n",
    "                # getting new links from current article\n",
    "                new_link = str(next_article.get(\"href\"))\n",
    "                # sometimes, there is a source page after the \"?\" leading to duplicates in the link\n",
    "                if new_link.find(\"?\") != -1:\n",
    "                    # removing part after the \"?\"\n",
    "                    new_link = new_link[:new_link.find(\"?\")]\n",
    "                if new_link.find(\"pravda.sk\") == -1:\n",
    "                    new_link = \"https://spravy.pravda.sk/\" + new_link\n",
    "                # if new_link is not in the list, then it will be added\n",
    "                if new_link not in article_links:\n",
    "                    article_links.append(new_link)\n",
    "                    \n",
    "            #identification of next articles:\n",
    "            for next_article in body.select('aside.hp-box-najcitanejsie li a'):\n",
    "                new_link = str(next_article.get(\"href\"))\n",
    "                if new_link.find(\"?\") != -1:\n",
    "                    new_link = new_link[:new_link.find(\"?\")]\n",
    "                if new_link.find(\"pravda.sk\") == -1:\n",
    "                    new_link = \"https://spravy.pravda.sk/\" + new_link\n",
    "                \n",
    "                if new_link not in article_links:\n",
    "                    article_links.append(new_link)\n",
    "            \n",
    "            i = i + 1\n",
    "            self.scraped_links.append(link)\n",
    "            print \"Done!!\"\n",
    "            print \"\"\n",
    "            \n",
    "            # Saving to disk temp results\n",
    "            if (i%100 == 0) & (len(self.sme_articles) > 1):\n",
    "                print \"Saving partial results to disk!\"\n",
    "                df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "                df_articles.columns = [\"label\",\"summary\",\"text\"]\n",
    "                df_articles.to_csv(data_path + \"temp_pravda_articles.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "                df_links = pd.DataFrame(self.scraped_links)\n",
    "                df_links.columns = [\"links\"]\n",
    "                df_links.to_csv(data_path + \"temp_pravda_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        \n",
    "        # Creating final dataframe and saving to disk\n",
    "        # TODO better savings\n",
    "        if len(self.sme_articles) > 0:\n",
    "            postfix = str(time.time())\n",
    "            if postfix.find(\".\") != -1:\n",
    "                postfix = postfix[:postfix.find(\".\")]\n",
    "            \n",
    "            print \"Saving final results to disk!\"\n",
    "            df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "            df_articles.columns = [\"label\",\"summary\",\"text\"]\n",
    "            df_articles.to_csv(data_path + \"pravda_articles\"+ postfix + \".csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "            df_links = pd.DataFrame(self.scraped_links)\n",
    "            df_links.columns = [\"links\"]\n",
    "            df_links.to_csv(data_path + \"pravda_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        else:\n",
    "            print \"No new articles scraped\"\n",
    "        return self.sme_articles, self.scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = ScrapePravda()\n",
    "# articles, links = x.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/Users/jurajkapasny/Data/sk_text_for_api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    scraper = ScrapePravda()\n",
    "    articles, links = scraper.scrape()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    minutes = int(elapsed / 60)\n",
    "    seconds = elapsed % 60\n",
    "    print \"Number of scraped articles:\", len(articles)\n",
    "    print \"Time Elapsed:\", minutes, \"minutes and\", seconds, \"seconds\"\n",
    "    print \"=================================\"\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this_lin = pd.read_csv(data_path+\"temp_pravda_links.csv\", sep =\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def find_body(soup):\n",
    "#     main_content = list(soup.children)\n",
    "#     for i in main_content:\n",
    "#         #ked zacina cast <html tak to je main part\n",
    "#         if str(i).find(\"<html\") == 0:\n",
    "#             inner_content = i\n",
    "#             for j in inner_content:\n",
    "#                 if str(j).find(\"<body\") == 0:\n",
    "#                     body = j\n",
    "#                     print \"Find HTML <body>: Successfull!!\"\n",
    "#     return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# link = links.links[751]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # sometimes link doesn't work\n",
    "# try:\n",
    "#     article = requests.get(link)\n",
    "# except:\n",
    "#     print \"link was corrupted\"\n",
    "#     print \"\"\n",
    "\n",
    "\n",
    "\n",
    "# local_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "# # find HTML <body> tag\n",
    "# body = find_body(soup = local_soup)\n",
    "\n",
    "# # label is group of news from webpage\n",
    "# # label is group of news from webpage\n",
    "# try:\n",
    "#     label = re.search('.pravda.sk/(.*?)/', link).group(1)\n",
    "# except AttributeError:\n",
    "#     label = \"\"\n",
    "\n",
    "# # summary is in the first paragraph\n",
    "# for desc in body.select('article.article-detail div.article-detail-perex p'):\n",
    "#      summary = desc.get_text()\n",
    "# # identification of text of the article\n",
    "# text = \"\"\n",
    "# for article_part in body.select('article.article-detail div.article-detail-body p'):\n",
    "#      text = text + \"\\n\" + article_part.get_text()\n",
    "\n",
    "# article_links = []\n",
    "# #identification of next articles:\n",
    "# for next_article in body.select('aside.article-detail-body-odporucame div.article-head h3 a'):\n",
    "#     # getting new links from current article\n",
    "#     new_link = str(next_article.get(\"href\"))\n",
    "#     # sometimes, there is a source page after the \"?\" leading to duplicates in the link\n",
    "#     if new_link.find(\"?\") != -1:\n",
    "#         # removing part after the \"?\"\n",
    "#         new_link = new_link[:new_link.find(\"?\")]\n",
    "#     if new_link.find(\"pravda.sk\") == -1:\n",
    "#         new_link = \"https://spravy.pravda.sk/\" + new_link\n",
    "#     # if new_link is not in the list, then it will be added\n",
    "#     if new_link not in article_links:\n",
    "#         article_links.append(new_link)\n",
    "\n",
    "# #identification of next articles:\n",
    "# for next_article in body.select('aside.hp-box-najcitanejsie li a'):\n",
    "#     new_link = str(next_article.get(\"href\"))\n",
    "#     if new_link.find(\"?\") != -1:\n",
    "#         new_link = new_link[:new_link.find(\"?\")]\n",
    "#     if new_link.find(\"pravda.sk\") == -1:\n",
    "#         new_link = \"https://spravy.pravda.sk/\" + new_link\n",
    "\n",
    "#     if new_link not in article_links:\n",
    "#         article_links.append(new_link)\n",
    "\n",
    "# print text\n",
    "# print \"\"\n",
    "# print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
