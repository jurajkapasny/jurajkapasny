{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "data_path = \"/Users/jurajkapasny/Data/sk_text_for_api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ScrapeSME:\n",
    "    def __init__(self):\n",
    "        # Base soup object\n",
    "        self.page = requests.get(\"http://www.sme.sk\")\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        \n",
    "        # I will be adding links here constantly so needs to be class variable\n",
    "        # loading old links from disk\n",
    "        try:\n",
    "            self.scraped_links = list(pd.read_csv(data_path + \"sme_links.csv\", sep = \"|\").links.values)\n",
    "            print len(self.scraped_links), \"links loaded from disk\"\n",
    "        except IOError:    \n",
    "            # Empty link list if no data are on the disk\n",
    "            self.scraped_links = []\n",
    "    \n",
    "    def find_body(self, soup = None):\n",
    "        if soup is None:\n",
    "            soup = self.soup\n",
    "        main_content = list(soup.children)\n",
    "        for i in main_content:\n",
    "            #ked zacina cast <html tak to je main part\n",
    "            if str(i).find(\"<html\") == 0:\n",
    "                inner_content = i\n",
    "                for j in inner_content:\n",
    "                    if str(j).find(\"<body\") == 0:\n",
    "                        body = j\n",
    "                        print \"Find HTML <body>: Successfull!!\"\n",
    "        return body\n",
    "    \n",
    "    def find_initial_links(self):\n",
    "        article_links = []\n",
    "        body = self.find_body()\n",
    "        for i in body.select('div.col-responsive div a.js-smphr-item'):\n",
    "            link = str(i.get(\"href\"))\n",
    "            if link.find(\"?\") != -1:\n",
    "                link = link[:link.find(\"?\")]\n",
    "            article_links.append(link)\n",
    "        return article_links\n",
    "            \n",
    "    def scrape(self):\n",
    "        # initial links to scrape\n",
    "        article_links = self.find_initial_links()\n",
    "        self.sme_articles = {}\n",
    "        i = 0\n",
    "        while i < len(article_links):\n",
    "            print \"Number of articles for download:\",len(article_links)\n",
    "            print \"Scraping article number\", i+1\n",
    "            link = article_links[i]\n",
    "            # skip this interation if link is already parsed\n",
    "            if link in self.scraped_links:\n",
    "                print \"Link already scraped, skipping...!\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                continue\n",
    "            \n",
    "            # sometimes link doesn't work\n",
    "            try:\n",
    "                article = requests.get(link)\n",
    "            except:\n",
    "                print \"link was corrupted\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                # skips rest of the loop\n",
    "                continue\n",
    "            \n",
    "            local_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "            # find HTML <body> tag\n",
    "            body = self.find_body(soup = local_soup)\n",
    "            \n",
    "            # label is group of news from webpage\n",
    "            label = re.search('//(.*).sme.sk', link).group(1)\n",
    "            text = \"\"\n",
    "            # identification of text of the article\n",
    "            for article_part in body.select('article p'):\n",
    "                 text = text + \"\\n\" + article_part.get_text()\n",
    "            self.sme_articles[i] = (label, text)\n",
    "            \n",
    "            #identification of next articles:\n",
    "            for next_article in body.select('div.media-body h2.media-heading a'):\n",
    "                # getting new links from current article\n",
    "                new_link = str(next_article.get(\"href\"))\n",
    "                # sometimes, there is a source page after the \"?\" leading to duplicates in the link\n",
    "                if new_link.find(\"?\") != -1:\n",
    "                    # removing part after the \"?\"\n",
    "                    new_link = new_link[:new_link.find(\"?\")]\n",
    "                # if new_link is not in the list, then it will be added\n",
    "                if new_link not in article_links:\n",
    "                    article_links.append(new_link)\n",
    "                    \n",
    "            #identification of next articles:\n",
    "            for next_article in body.select('div.cr-box div.cr-content h4 a'):\n",
    "                new_link = str(next_article.get(\"href\"))\n",
    "                if new_link.find(\"?\") != -1:\n",
    "                    new_link = new_link[:new_link.find(\"?\")]\n",
    "                if new_link not in article_links:\n",
    "                    article_links.append(new_link)\n",
    "            \n",
    "            i = i + 1\n",
    "            self.scraped_links.append(link)\n",
    "            print \"Done!!\"\n",
    "            print \"\"\n",
    "            \n",
    "            # Saving to disk temp results\n",
    "            if (i%100 == 0) & (len(self.sme_articles) > 1):\n",
    "                print \"Saving partial results to disk!\"\n",
    "                df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "                df_articles.columns = [\"category\",\"text\"]\n",
    "                df_articles.to_csv(data_path + \"temp_sme_articles.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "                df_links = pd.DataFrame(self.scraped_links)\n",
    "                df_links.columns = [\"links\"]\n",
    "                df_links.to_csv(data_path + \"temp_sme_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        \n",
    "        # Creating final dataframe and saving to disk\n",
    "        # TODO better savings\n",
    "        if len(self.sme_articles) > 1:\n",
    "            postfix = str(time.time())\n",
    "            if postfix.find(\".\") != -1:\n",
    "                postfix = postfix[:postfix.find(\".\")]\n",
    "            \n",
    "            print \"Saving final results to disk!\"\n",
    "            df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "            df_articles.columns = [\"category\",\"text\"]\n",
    "            df_articles.to_csv(data_path + \"sme_articles\"+ postfix + \".csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "            df_links = pd.DataFrame(self.scraped_links)\n",
    "            df_links.columns = [\"links\"]\n",
    "            df_links.to_csv(data_path + \"sme_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        else:\n",
    "            print \"No new articles scraped\"\n",
    "        \n",
    "        return self.sme_articles, self.scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204 links loaded from disk\n",
      "Find HTML <body>: Successfull!!\n",
      "Number of articles for download: 8\n",
      "Scraping article number 1\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 24\n",
      "Scraping article number 2\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 36\n",
      "Scraping article number 3\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 36\n",
      "Scraping article number 4\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 36\n",
      "Scraping article number 5\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 39\n",
      "Scraping article number 6\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 39\n",
      "Scraping article number 7\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 8\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 9\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 10\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 11\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 12\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 13\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 14\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 15\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 41\n",
      "Scraping article number 16\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 17\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 18\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 19\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 20\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 21\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 22\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 23\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 45\n",
      "Scraping article number 24\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 25\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 26\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 27\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 28\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 29\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 30\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 31\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 32\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 33\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 34\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 35\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 36\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 37\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 38\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 39\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 40\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 41\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 49\n",
      "Scraping article number 42\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 43\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 44\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 45\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 46\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 47\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 48\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 49\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 50\n",
      "Scraping article number 50\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Saving final results to disk!\n"
     ]
    }
   ],
   "source": [
    "x = ScrapeSME()\n",
    "articles, links = x.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
