{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "data_path = \"/Users/jurajkapasny/Data/sk_text_for_api/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ScrapeDennikn:\n",
    "    def __init__(self):\n",
    "        # Base soup object\n",
    "        self.page = requests.get(\"http://www.dennikn.sk\")\n",
    "        self.soup = BeautifulSoup(self.page.content, 'html.parser')\n",
    "        \n",
    "        # I will be adding links here constantly so needs to be class variable\n",
    "        # loading old links from disk\n",
    "        try:\n",
    "            self.scraped_links = list(pd.read_csv(data_path + \"dennikn_links.csv\", sep = \"|\").links.values)\n",
    "            print len(self.scraped_links), \"links loaded from disk\"\n",
    "        except IOError:    \n",
    "            # Empty link list if no data are on the disk\n",
    "            self.scraped_links = []\n",
    "    \n",
    "    def find_body(self, soup = None):\n",
    "        if soup is None:\n",
    "            soup = self.soup\n",
    "        main_content = list(soup.children)\n",
    "        for i in main_content:\n",
    "#             try:\n",
    "#                 string = str(i)\n",
    "#             except UnicodeEncodeError:\n",
    "            string = i.encode('utf-8')\n",
    "            #ked zacina cast <html tak to je main part\n",
    "            if string.find(\"<html\") == 0:\n",
    "                inner_content = i\n",
    "                for j in inner_content:\n",
    "                    inner_string = j.encode('utf-8')\n",
    "                    if inner_string.find(\"<body\") == 0:\n",
    "                        body = j\n",
    "                        print \"Find HTML <body>: Successfull!!\"\n",
    "        if 'body' not in locals():\n",
    "            print \"body wasn't find\"\n",
    "            body = None\n",
    "        return body\n",
    "    \n",
    "    def find_initial_links(self):\n",
    "        article_links = []\n",
    "        summaries = []\n",
    "        body = self.find_body()\n",
    "        # <p> is paragraph over the <a> so we have access to additional text\n",
    "        for i in body.select('section.b div.a_articles article.a_minute p'):\n",
    "            # link is always first elementh in the paragraph, sometimes is missings (taking from minuta po minute)\n",
    "            try:\n",
    "                link = str(i.select('a')[0].get(\"href\"))\n",
    "            except IndexError:\n",
    "                continue\n",
    "            if link.find(\"?\") != -1:\n",
    "                link = link[:link.find(\"?\")]\n",
    "            summaries.append(i.get_text())\n",
    "            article_links.append(link)\n",
    "    \n",
    "        return article_links, summaries\n",
    "            \n",
    "    def scrape(self):\n",
    "        # initial links to scrape\n",
    "        article_links, summaries = self.find_initial_links()\n",
    "        self.sme_articles = {}\n",
    "        i = 0\n",
    "        while i < len(article_links):\n",
    "        #while i < 2:\n",
    "            print \"Number of articles for download:\",len(article_links)\n",
    "            print \"Scraping article number\", i+1\n",
    "            link = article_links[i]\n",
    "            print link\n",
    "            summary = summaries[i]\n",
    "            # skip this interation if link is already parsed\n",
    "            if link in self.scraped_links:\n",
    "                print \"Link already scraped, skipping...!\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                continue\n",
    "            \n",
    "            # sometimes link doesn't work\n",
    "            try:\n",
    "                article = requests.get(link)\n",
    "            except:\n",
    "                print \"link was corrupted\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                # skips rest of the loop\n",
    "                continue\n",
    "            \n",
    "            local_soup = BeautifulSoup(article.content, 'html.parser')\n",
    "            # find HTML <body> tag\n",
    "            body = self.find_body(soup = local_soup)\n",
    "            if body == None:\n",
    "                print \"link was corrupted\"\n",
    "                print \"\"\n",
    "                i = i + 1\n",
    "                # skips rest of the loop\n",
    "                continue\n",
    "            \n",
    "            # label is group of news from webpage\n",
    "            try:\n",
    "                label = re.search('//(.*).dennikn.sk', link).group(1)\n",
    "            except AttributeError:\n",
    "                label = \"\"\n",
    "            text = \"\"\n",
    "            # identification of text of the article\n",
    "            for article_part in body.select('article p'):\n",
    "                 text = text + \"\\n\" + article_part.get_text()\n",
    "            self.sme_articles[i] = (label,summary, text)\n",
    "            \n",
    "            #identification of next articles:\n",
    "            for next_article in body.select('div.a_articles article.a_medium div.text h3.title a'):\n",
    "                # getting new links from current article\n",
    "                new_link = str(next_article.get(\"href\"))\n",
    "                # sometimes, there is a source page after the \"?\" leading to duplicates in the link\n",
    "                if new_link.find(\"?\") != -1:\n",
    "                    # removing part after the \"?\"\n",
    "                    new_link = new_link[:new_link.find(\"?\")]\n",
    "                new_summary = next_article.get_text()\n",
    "                # if new_link is not in the list, then it will be added\n",
    "                if new_link not in article_links:\n",
    "                    article_links.append(new_link)\n",
    "                    summaries.append(new_summary)\n",
    "                    \n",
    "            #identification of next articles:\n",
    "            for next_article in body.select('section.b div.a_articles article.a_popular a'):\n",
    "                new_link = str(next_article.get(\"href\"))\n",
    "                if new_link.find(\"?\") != -1:\n",
    "                    new_link = new_link[:new_link.find(\"?\")]\n",
    "                new_summary = next_article.select('h3')[0].get_text()\n",
    "                if new_link not in article_links:\n",
    "                    article_links.append(new_link)\n",
    "                    summaries.append(new_summary)\n",
    "            \n",
    "            i = i + 1\n",
    "            self.scraped_links.append(link)\n",
    "            print \"Done!!\"\n",
    "            print \"\"\n",
    "            \n",
    "#             # Saving to disk temp results\n",
    "#             if (i%100 == 0) & (len(self.sme_articles) > 1):\n",
    "#                 print \"Saving partial results to disk!\"\n",
    "#                 df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "#                 df_articles.columns = [\"label\",\"summary\",\"text\"]\n",
    "#                 df_articles.to_csv(data_path + \"temp_dennikn_articles.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "#                 df_links = pd.DataFrame(self.scraped_links)\n",
    "#                 df_links.columns = [\"links\"]\n",
    "#                 df_links.to_csv(data_path + \"temp_dennikn_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        \n",
    "        # Creating final dataframe and saving to disk\n",
    "        # TODO better savings\n",
    "        if len(self.sme_articles) > 0:\n",
    "            postfix = str(time.time())\n",
    "            if postfix.find(\".\") != -1:\n",
    "                postfix = postfix[:postfix.find(\".\")]\n",
    "            \n",
    "            print \"Saving final results to disk!\"\n",
    "            df_articles = pd.DataFrame(self.sme_articles).transpose()\n",
    "            df_articles.columns = [\"label\",\"summary\",\"text\"]\n",
    "            df_articles.to_csv(data_path + \"dennikn_articles\"+ postfix + \".csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "            df_links = pd.DataFrame(self.scraped_links)\n",
    "            df_links.columns = [\"links\"]\n",
    "            df_links.to_csv(data_path + \"dennikn_links.csv\", sep = \"|\", index=False, encoding=\"utf-8\")\n",
    "        else:\n",
    "            print \"No new articles scraped\"\n",
    "        \n",
    "        return self.sme_articles, self.scraped_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x = ScrapeDennikn()\n",
    "# articles, links = x.scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 links loaded from disk\n",
      "Find HTML <body>: Successfull!!\n",
      "Number of articles for download: 54\n",
      "Scraping article number 1\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 54\n",
      "Scraping article number 2\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 54\n",
      "Scraping article number 3\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 74\n",
      "Scraping article number 4\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 74\n",
      "Scraping article number 5\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 74\n",
      "Scraping article number 6\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 74\n",
      "Scraping article number 7\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 8\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 9\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 10\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 11\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 12\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 13\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 14\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 15\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 16\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 17\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 18\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 19\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 20\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 21\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 78\n",
      "Scraping article number 22\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 23\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 24\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 25\n",
      "Find HTML <body>: Successfull!!\n",
      "link was corrupted\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 26\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 27\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 28\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 29\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 30\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 31\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 32\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 33\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 34\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 35\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 36\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 37\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 38\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 39\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 40\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 41\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 42\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 43\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 44\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 45\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 46\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 47\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 48\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 49\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 50\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 51\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 52\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 53\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 54\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 55\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 56\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 57\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 58\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 59\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 60\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 61\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 62\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 79\n",
      "Scraping article number 63\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 64\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 65\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 66\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 67\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 68\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 69\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 70\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 71\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 72\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 73\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 74\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 75\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 76\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 77\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 78\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 79\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 80\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 81\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 82\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 83\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 84\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 85\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 86\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 87\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 88\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 89\n",
      "Find HTML <body>: Successfull!!\n",
      "Done!!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 90\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 91\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 92\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 93\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 94\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 95\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Number of articles for download: 96\n",
      "Scraping article number 96\n",
      "Link already scraped, skipping...!\n",
      "\n",
      "Saving final results to disk!\n",
      "Number of scraped articles: 31\n",
      "Time Elapsed: 3 minutes and 24.8363029957 seconds\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    scraper = ScrapeDennikn()\n",
    "    articles, links = scraper.scrape()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    minutes = int(elapsed / 60)\n",
    "    seconds = elapsed % 60\n",
    "    print \"Number of scraped articles:\", len(articles)\n",
    "    print \"Time Elapsed:\", minutes, \"minutes and\", seconds, \"seconds\"\n",
    "    print \"=================================\"\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
